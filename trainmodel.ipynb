{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8d0d8a4-40fd-4fb7-8638-c6cd14e6a668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02e9c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'images/train'\n",
    "TEST_DIR = 'images/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2c51fa-17bc-4981-8851-c4c36cd19385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createdataframe(dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label in os.listdir(dir):\n",
    "        for imagename in os.listdir(os.path.join(dir,label)):\n",
    "            image_paths.append(os.path.join(dir,label,imagename))\n",
    "            labels.append(label)\n",
    "        print(label, \"completed\")\n",
    "    return image_paths,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3a8f1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    }
   ],
   "source": [
    "train = pd.DataFrame()\n",
    "train['image'], train['label'] = createdataframe(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8a0cd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                image     label\n",
      "0            images/train\\angry\\0.jpg     angry\n",
      "1            images/train\\angry\\1.jpg     angry\n",
      "2           images/train\\angry\\10.jpg     angry\n",
      "3        images/train\\angry\\10002.jpg     angry\n",
      "4        images/train\\angry\\10016.jpg     angry\n",
      "...                               ...       ...\n",
      "28816  images/train\\surprise\\9969.jpg  surprise\n",
      "28817  images/train\\surprise\\9985.jpg  surprise\n",
      "28818  images/train\\surprise\\9990.jpg  surprise\n",
      "28819  images/train\\surprise\\9992.jpg  surprise\n",
      "28820  images/train\\surprise\\9996.jpg  surprise\n",
      "\n",
      "[28821 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f9688fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    }
   ],
   "source": [
    "test = pd.DataFrame()\n",
    "test['image'], test['label'] = createdataframe(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f504f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              image     label\n",
      "0       images/test\\angry\\10052.jpg     angry\n",
      "1       images/test\\angry\\10065.jpg     angry\n",
      "2       images/test\\angry\\10079.jpg     angry\n",
      "3       images/test\\angry\\10095.jpg     angry\n",
      "4       images/test\\angry\\10121.jpg     angry\n",
      "...                             ...       ...\n",
      "7061  images/test\\surprise\\9806.jpg  surprise\n",
      "7062  images/test\\surprise\\9830.jpg  surprise\n",
      "7063  images/test\\surprise\\9853.jpg  surprise\n",
      "7064  images/test\\surprise\\9878.jpg  surprise\n",
      "7065   images/test\\surprise\\993.jpg  surprise\n",
      "\n",
      "[7066 rows x 2 columns]\n",
      "0         images/test\\angry\\10052.jpg\n",
      "1         images/test\\angry\\10065.jpg\n",
      "2         images/test\\angry\\10079.jpg\n",
      "3         images/test\\angry\\10095.jpg\n",
      "4         images/test\\angry\\10121.jpg\n",
      "                    ...              \n",
      "7061    images/test\\surprise\\9806.jpg\n",
      "7062    images/test\\surprise\\9830.jpg\n",
      "7063    images/test\\surprise\\9853.jpg\n",
      "7064    images/test\\surprise\\9878.jpg\n",
      "7065     images/test\\surprise\\993.jpg\n",
      "Name: image, Length: 7066, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "693324b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f52ccbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(images):\n",
    "    features = []\n",
    "    for image in tqdm(images):\n",
    "        img = load_img(image, color_mode=\"grayscale\", target_size=(48,48))\n",
    "        img = np.array(img)\n",
    "        features.append(img)\n",
    "    features = np.array(features)\n",
    "    features = features.reshape(len(features),48,48,1)\n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "942a9d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420a7783dfc8406581caba9244249fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28821 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_features = extract_features(train['image']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1b294ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c57d44092b24036b30bb1ad4414af73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7066 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_features = extract_features(test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f78b5d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_features/255.0\n",
    "x_test = test_features/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c660fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5384a25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LabelEncoder</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.LabelEncoder.html\">?<span>Documentation for LabelEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LabelEncoder()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50ece229",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = le.transform(train['label'])\n",
    "y_test = le.transform(test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e284299",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train,num_classes = 7)\n",
    "y_test = to_categorical(y_test,num_classes = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3977223a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shivam goyal\\downloads\\face_emotion_recognition_machine_learning-main\\.venv\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# convolutional layers\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "# fully connected layers\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "# output layer\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de986d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5cd0b130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m 32/226\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:39\u001b[0m 2s/step - accuracy: 0.4487 - loss: 1.4108"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[1;32mc:\\users\\shivam goyal\\downloads\\face_emotion_recognition_machine_learning-main\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\users\\shivam goyal\\downloads\\face_emotion_recognition_machine_learning-main\\.venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    376\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 377\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\users\\shivam goyal\\downloads\\face_emotion_recognition_machine_learning-main\\.venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    218\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    219\u001b[0m     ):\n\u001b[1;32m--> 220\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    222\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\shivam goyal\\downloads\\face_emotion_recognition_machine_learning-main\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\users\\shivam goyal\\downloads\\face_emotion_recognition_machine_learning-main\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\users\\shivam goyal\\downloads\\face_emotion_recognition_machine_learning-main\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\shivam goyal\\downloads\\face_emotion_recognition_machine_learning-main\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\shivam goyal\\downloads\\face_emotion_recognition_machine_learning-main\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\users\\shivam goyal\\downloads\\face_emotion_recognition_machine_learning-main\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\users\\shivam goyal\\downloads\\face_emotion_recognition_machine_learning-main\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\users\\shivam goyal\\downloads\\face_emotion_recognition_machine_learning-main\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32mc:\\users\\shivam goyal\\downloads\\face_emotion_recognition_machine_learning-main\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x= x_train,y = y_train, batch_size = 128, epochs = 100, validation_data = (x_test,y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0f1923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"emotiondetector.json\",'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save(\"emotiondetector.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b6563-9dbe-436b-8504-64744a32d4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dont run if we trained the model on your own\n",
    "# from tensorflow.keras.models import Sequential, model_from_json\n",
    "\n",
    "# with open(\"facialemotionmodel.json\", \"r\") as json_file:\n",
    "#     model_json = json_file.read()\n",
    "\n",
    "# model = model_from_json(model_json, custom_objects={\"Sequential\": Sequential})\n",
    "\n",
    "# # Load weights\n",
    "# model.load_weights(\"facialemotionmodel.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3932bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label = ['angry','disgust','fear','happy','neutral','sad','surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ccbacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ef(image):\n",
    "#     img = load_img(image, color_mode=\"grayscale\", target_size=(48,48))\n",
    "#     feature = np.array(img)\n",
    "#     feature = feature.reshape(1,48,48,1)\n",
    "#     return feature/255.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83af9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is of sad\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 749ms/step\n",
      "model prediction is  sad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a1b7ceec40>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# image = 'images/train/sad/42.jpg'\n",
    "# print(\"original image is of sad\")\n",
    "# img = ef(image)\n",
    "# pred = model.predict(img)\n",
    "# pred_label = label[pred.argmax()]\n",
    "# print(\"model prediction is \",pred_label)\n",
    "# plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
